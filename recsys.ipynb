{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T04:19:54.714342Z","iopub.status.busy":"2024-04-30T04:19:54.713732Z","iopub.status.idle":"2024-04-30T04:20:07.474835Z","shell.execute_reply":"2024-04-30T04:20:07.473686Z","shell.execute_reply.started":"2024-04-30T04:19:54.714303Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pytorch-pretrained-bert in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.6.2)\n","Requirement already satisfied: torch>=0.4.1 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytorch-pretrained-bert) (2.3.0)\n","Requirement already satisfied: numpy in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytorch-pretrained-bert) (1.26.4)\n","Requirement already satisfied: boto3 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytorch-pretrained-bert) (1.34.94)\n","Requirement already satisfied: requests in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytorch-pretrained-bert) (2.31.0)\n","Requirement already satisfied: tqdm in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytorch-pretrained-bert) (4.66.2)\n","Requirement already satisfied: regex in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytorch-pretrained-bert) (2024.4.28)\n","Requirement already satisfied: filelock in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.14.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=0.4.1->pytorch-pretrained-bert) (4.11.0)\n","Requirement already satisfied: sympy in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=0.4.1->pytorch-pretrained-bert) (1.12)\n","Requirement already satisfied: networkx in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.3)\n","Requirement already satisfied: jinja2 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.1.3)\n","Requirement already satisfied: fsspec in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=0.4.1->pytorch-pretrained-bert) (2024.3.1)\n","Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=0.4.1->pytorch-pretrained-bert) (2021.4.0)\n","Requirement already satisfied: botocore<1.35.0,>=1.34.94 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from boto3->pytorch-pretrained-bert) (1.34.94)\n","Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from boto3->pytorch-pretrained-bert) (1.0.1)\n","Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from boto3->pytorch-pretrained-bert) (0.10.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->pytorch-pretrained-bert) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->pytorch-pretrained-bert) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->pytorch-pretrained-bert) (2.2.1)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->pytorch-pretrained-bert) (2024.2.2)\n","Requirement already satisfied: colorama in c:\\users\\yashk\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->pytorch-pretrained-bert) (0.4.6)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\yashk\\appdata\\roaming\\python\\python312\\site-packages (from botocore<1.35.0,>=1.34.94->boto3->pytorch-pretrained-bert) (2.9.0.post0)\n","Requirement already satisfied: intel-openmp==2021.* in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=0.4.1->pytorch-pretrained-bert) (2021.4.0)\n","Requirement already satisfied: tbb==2021.* in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=0.4.1->pytorch-pretrained-bert) (2021.12.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch>=0.4.1->pytorch-pretrained-bert) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch>=0.4.1->pytorch-pretrained-bert) (1.3.0)\n","Requirement already satisfied: six>=1.5 in c:\\users\\yashk\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.94->boto3->pytorch-pretrained-bert) (1.16.0)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install pytorch-pretrained-bert"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T04:20:07.477286Z","iopub.status.busy":"2024-04-30T04:20:07.476970Z","iopub.status.idle":"2024-04-30T04:20:07.481771Z","shell.execute_reply":"2024-04-30T04:20:07.480722Z","shell.execute_reply.started":"2024-04-30T04:20:07.477256Z"},"trusted":true},"outputs":[],"source":["# !pip install --upgrade tensorflow-addons"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T04:20:07.483389Z","iopub.status.busy":"2024-04-30T04:20:07.483028Z","iopub.status.idle":"2024-04-30T04:20:07.490026Z","shell.execute_reply":"2024-04-30T04:20:07.489007Z","shell.execute_reply.started":"2024-04-30T04:20:07.483344Z"},"trusted":true},"outputs":[],"source":["# !pip install 'keras<3.0.0' mediapipe-model-maker"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T04:20:07.491627Z","iopub.status.busy":"2024-04-30T04:20:07.491322Z","iopub.status.idle":"2024-04-30T04:21:12.769506Z","shell.execute_reply":"2024-04-30T04:21:12.768369Z","shell.execute_reply.started":"2024-04-30T04:20:07.491601Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: tensorflow in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.16.1)\n","Requirement already satisfied: tensorflow-intel==2.16.1 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (2.16.1)\n","Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.1.0)\n","Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (24.3.25)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.5.4)\n","Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.2.0)\n","Requirement already satisfied: h5py>=3.10.0 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.11.0)\n","Requirement already satisfied: libclang>=13.0.0 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (18.1.1)\n","Requirement already satisfied: ml-dtypes~=0.3.1 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.3.2)\n","Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.3.0)\n","Requirement already satisfied: packaging in c:\\users\\yashk\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (24.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.25.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.31.0)\n","Requirement already satisfied: setuptools in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (69.5.1)\n","Requirement already satisfied: six>=1.12.0 in c:\\users\\yashk\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.11.0)\n","Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.62.2)\n","Requirement already satisfied: tensorboard<2.17,>=2.16 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.16.2)\n","Requirement already satisfied: keras>=3.0.0 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.3.3)\n","Requirement already satisfied: numpy<2.0.0,>=1.26.0 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.26.4)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow) (0.43.0)\n","Requirement already satisfied: rich in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (13.7.1)\n","Requirement already satisfied: namex in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.0.8)\n","Requirement already satisfied: optree in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.11.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2.2.1)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2024.2.2)\n","Requirement already satisfied: markdown>=2.6.8 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.6)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.0.2)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.1.5)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\yashk\\appdata\\roaming\\python\\python312\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (2.17.2)\n","Requirement already satisfied: mdurl~=0.1 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.1.2)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install --upgrade tensorflow"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T04:21:12.772738Z","iopub.status.busy":"2024-04-30T04:21:12.772414Z","iopub.status.idle":"2024-04-30T04:21:12.779158Z","shell.execute_reply":"2024-04-30T04:21:12.776172Z","shell.execute_reply.started":"2024-04-30T04:21:12.772709Z"},"trusted":true},"outputs":[],"source":["# !pip install --upgrade keras"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T04:21:12.781127Z","iopub.status.busy":"2024-04-30T04:21:12.780762Z","iopub.status.idle":"2024-04-30T04:21:12.831685Z","shell.execute_reply":"2024-04-30T04:21:12.830638Z","shell.execute_reply.started":"2024-04-30T04:21:12.781093Z"},"trusted":true},"outputs":[],"source":["# import keras\n","# from keras.models import load_model"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T04:21:12.833409Z","iopub.status.busy":"2024-04-30T04:21:12.833033Z","iopub.status.idle":"2024-04-30T04:21:12.840183Z","shell.execute_reply":"2024-04-30T04:21:12.839284Z","shell.execute_reply.started":"2024-04-30T04:21:12.833375Z"},"trusted":true},"outputs":[],"source":["# !pip install tensorflow_addons"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T04:21:12.842215Z","iopub.status.busy":"2024-04-30T04:21:12.841846Z","iopub.status.idle":"2024-04-30T04:21:12.849026Z","shell.execute_reply":"2024-04-30T04:21:12.847946Z","shell.execute_reply.started":"2024-04-30T04:21:12.842182Z"},"trusted":true},"outputs":[],"source":["# pip install tensorflow -"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T04:21:12.850591Z","iopub.status.busy":"2024-04-30T04:21:12.850285Z","iopub.status.idle":"2024-04-30T04:22:00.871982Z","shell.execute_reply":"2024-04-30T04:22:00.870832Z","shell.execute_reply.started":"2024-04-30T04:21:12.850564Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Note: you may need to restart the kernel to use updated packages.\n"]},{"name":"stderr","output_type":"stream","text":["The system cannot find the file specified.\n"]}],"source":["%pip install 'keras<3.0.0' mediapipe-model-maker"]},{"cell_type":"code","execution_count":136,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T04:22:00.874102Z","iopub.status.busy":"2024-04-30T04:22:00.873754Z","iopub.status.idle":"2024-04-30T04:22:10.689661Z","shell.execute_reply":"2024-04-30T04:22:10.688906Z","shell.execute_reply.started":"2024-04-30T04:22:00.874051Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torchvision.utils as vutils\n","import torchvision.transforms as transforms\n","import os\n","import sys\n","input_dir = r'D:\\clg files\\RecSys\\py_files'\n","sys.path.append(input_dir)\n","\n","from data_util import AttDesDataset\n","from utils import weights_init\n","\n","from dcgan_model import Generator, Discriminator\n","import time\n","import imageio\n","import matplotlib.pyplot as plt\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":137,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T04:22:10.691416Z","iopub.status.busy":"2024-04-30T04:22:10.690874Z","iopub.status.idle":"2024-04-30T04:22:10.743811Z","shell.execute_reply":"2024-04-30T04:22:10.742709Z","shell.execute_reply.started":"2024-04-30T04:22:10.691388Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using Device cpu\n"]}],"source":["# Setting device to cuda\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(\"Using Device\", device)"]},{"cell_type":"code","execution_count":138,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T04:22:10.746034Z","iopub.status.busy":"2024-04-30T04:22:10.745559Z","iopub.status.idle":"2024-04-30T04:22:10.762543Z","shell.execute_reply":"2024-04-30T04:22:10.761792Z","shell.execute_reply.started":"2024-04-30T04:22:10.745986Z"},"trusted":true},"outputs":[],"source":["# directory to store output images\n","output_save_path = './generated_images/'\n","os.makedirs(output_save_path, exist_ok=True)"]},{"cell_type":"code","execution_count":139,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T04:22:10.764090Z","iopub.status.busy":"2024-04-30T04:22:10.763759Z","iopub.status.idle":"2024-04-30T04:22:10.770347Z","shell.execute_reply":"2024-04-30T04:22:10.769432Z","shell.execute_reply.started":"2024-04-30T04:22:10.764063Z"},"trusted":true},"outputs":[],"source":["# directory to store trained models\n","model_save_path = './saved_models/'\n","os.makedirs(model_save_path, exist_ok=True)"]},{"cell_type":"code","execution_count":140,"metadata":{},"outputs":[],"source":["from torch.utils.data import Dataset"]},{"cell_type":"code","execution_count":141,"metadata":{},"outputs":[],"source":["from torch.nn.utils.rnn import pad_sequence\n","\n","class CustomDataset(Dataset):\n","    def __init__(self, data):\n","        self.images = data['images']\n","        self.captions = data['captions']\n","\n","        # Build a vocabulary from the captions\n","        self.vocab = self.build_vocab(self.captions)\n","\n","    def build_vocab(self, captions):\n","        vocab = set()\n","        for caption in captions:\n","            vocab.update(list(caption))  # tokenize by character\n","        vocab = sorted(vocab)\n","        vocab.append('<unk>')  # add unknown token\n","        vocab.append('<pad>')  # add padding token\n","        vocab.append('<bos>')  # add beginning of sentence token\n","        vocab.append('<eos>')  # add end of sentence token\n","        word2idx = {word: idx for idx, word in enumerate(vocab)}\n","        return word2idx\n","\n","    def __getitem__(self, index):\n","        image = self.images[index]\n","        caption = self.captions[index]\n","\n","        # Tokenize the caption and convert it to tensor\n","        tokenized_caption = list(caption)  # tokenize by character\n","        caption_tensor = torch.tensor([self.vocab.get(token, self.vocab['<unk>']) for token in tokenized_caption])\n","\n","        return image, caption_tensor\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def collate_fn(self, data):\n","        # Sort the data in descending order of caption length\n","        data.sort(key=lambda x: len(x[1]), reverse=True)\n","\n","        images, captions = zip(*data)\n","\n","        # Pad the captions\n","        lengths = [len(cap) for cap in captions]\n","        targets = pad_sequence([torch.tensor(cap) for cap in captions], batch_first=True, padding_value=self.vocab['<pad>'])\n","\n","        return images, targets, lengths"]},{"cell_type":"code","execution_count":142,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T04:22:10.775506Z","iopub.status.busy":"2024-04-30T04:22:10.775153Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["1860\n","No of batches in train loader:  30\n","No of batches in validation loader:  6\n"]}],"source":["# data_root = r'/kaggle/input/textfor/data_for_test2.csv'\n","# split_root = ''\n","# dataset_name = 'Furniture'\n","# normalize = transforms.Normalize(mean=[0, 0, 0], std=[1, 1, 1])\n","# batch_size = 1 #128\n","# dataset = AttDesDataset(data_root, dataset_name, transform=transforms.Compose([\n","#                                             transforms.Resize((448,448)),\n","#                                             transforms.RandomHorizontalFlip(),\n","#                                             transforms.ToTensor(),\n","#                                             normalize,\n","#                                         ]))\n","\n","# print(dataset.images)\n","# print(dataset.des_list)\n","\n","# training_dict = {\n","#     'images': dataset.images,\n","#     'captions': dataset.des\n","# }\n","# train_loader = DataLoader(training_dict, batch_size=batch_size,shuffle=True,num_workers=8)\n","# print(\"No of batches: \",len(train_loader))\n","\n","from torch.utils.data import DataLoader, SubsetRandomSampler\n","import numpy as np\n","\n","data_root = r'data_files\\smoll.csv'\n","split_root = ''\n","dataset_name = 'Furniture'\n","normalize = transforms.Normalize(mean=[0, 0, 0], std=[1, 1, 1])\n","batch_size = 64 #128\n","dataset = AttDesDataset(data_root, dataset_name, transform=transforms.Compose([\n","                                            transforms.Resize((448,448)),\n","                                            transforms.RandomHorizontalFlip(),\n","                                            transforms.ToTensor(),\n","                                            normalize,\n","                                        ]))\n","\n","temp_dataset = {'images': dataset.images, 'captions': dataset.des_list}\n","custom_dataset = CustomDataset(temp_dataset)\n","print(len(custom_dataset))\n","# print(len(custom_dataset['captions']))\n","# Assuming your dataset has 1000 samples\n","num_samples = len(custom_dataset)\n","indices = list(range(num_samples))\n","split = int(np.floor(0.8 * num_samples))  # 80-20 train-validation split\n","\n","# Randomly shuffle the indices\n","np.random.shuffle(indices)\n","\n","# Split the indices into train and validation sets\n","train_indices, val_indices = indices[:split], indices[split:]\n","\n","# Define samplers for train and validation sets\n","train_sampler = SubsetRandomSampler(train_indices)\n","val_sampler = SubsetRandomSampler(val_indices)\n","\n","# Define DataLoader for train and validation sets using samplers\n","# train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler, num_workers=8)\n","train_loader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=custom_dataset.collate_fn)\n","val_loader = DataLoader(custom_dataset, batch_size=batch_size, sampler=val_sampler, num_workers=0, collate_fn=custom_dataset.collate_fn)\n","\n","# Check the number of batches in train and validation loaders\n","print(\"No of batches in train loader: \", len(train_loader))\n","print(\"No of batches in validation loader: \", len(val_loader))\n","\n"]},{"cell_type":"code","execution_count":143,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0\n","<class 'torch.Tensor'>\n","64\n","<class 'torch.Tensor'>\n","1\n","<class 'torch.Tensor'>\n","64\n","<class 'torch.Tensor'>\n"]}],"source":["for batch_idx,batch in enumerate(train_loader):\n","    print(batch_idx)\n","    # print(batch)\n","    print(type(batch[0][0]))\n","    print(len(batch[0]))\n","    print(type(batch[1]))\n","    if(batch_idx == 1):\n","        break"]},{"cell_type":"code","execution_count":144,"metadata":{"trusted":true},"outputs":[],"source":["# setting up parameters\n","noise_dim = 100\n","embed_dim = 256\n","embed_out_dim = 128\n","batch_size = 64 #128\n","real_label = 1.\n","fake_label = 0.\n","learning_rate = 0.0002\n","l1_coef = 50\n","l2_coef = 100\n","\n","num_epochs = 1\n","log_interval = 18 #43"]},{"cell_type":"code","execution_count":145,"metadata":{"trusted":true},"outputs":[],"source":["# loss functions\n","criterion = nn.BCELoss()\n","l2_loss = nn.MSELoss()\n","l1_loss = nn.L1Loss()"]},{"cell_type":"code","execution_count":146,"metadata":{"trusted":true},"outputs":[],"source":["# lists to store losses\n","D_losses = []\n","G_losses = []"]},{"cell_type":"code","execution_count":147,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["Generator(\n","  (text_embedding): Sequential(\n","    (0): Linear(in_features=256, out_features=128, bias=True)\n","    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n","  )\n","  (model): Sequential(\n","    (0): ConvTranspose2d(228, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n","    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU(inplace=True)\n","    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (5): ReLU(inplace=True)\n","    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (8): ReLU(inplace=True)\n","    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (11): ReLU(inplace=True)\n","    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (13): Tanh()\n","  )\n",")"]},"execution_count":147,"metadata":{},"output_type":"execute_result"}],"source":["# initializing generator\n","generator = Generator(channels=3, embed_dim=embed_dim, noise_dim=noise_dim, embed_out_dim=embed_out_dim).to(device)\n","generator.apply(weights_init)"]},{"cell_type":"code","execution_count":148,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["Discriminator(\n","  (model): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n","  )\n","  (text_embedding): Embedding(\n","    (text_embedding): Sequential(\n","      (0): Linear(in_features=256, out_features=128, bias=True)\n","      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): LeakyReLU(negative_slope=0.2, inplace=True)\n","    )\n","  )\n","  (output): Sequential(\n","    (0): Conv2d(640, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n","    (1): Sigmoid()\n","  )\n",")"]},"execution_count":148,"metadata":{},"output_type":"execute_result"}],"source":["# initializing discriminator\n","discriminator = Discriminator(channels=3, embed_dim=embed_dim, embed_out_dim=embed_out_dim).to(device)\n","discriminator.apply(weights_init)"]},{"cell_type":"code","execution_count":149,"metadata":{"trusted":true},"outputs":[],"source":["# setting up Adam optimizer for Generator and Discriminator\n","optimizer_G = torch.optim.Adam(generator.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n","optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=learning_rate, betas=(0.5, 0.999))"]},{"cell_type":"code","execution_count":150,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 1\n","epoch: 1, batch: 1/30\n","hi after images\n","hi after wrong images\n","embeddings size: torch.Size([64, 256])\n","training the descriminator\n","torch.Size([3, 256, 1, 1]) torch.Size([64, 256, 1, 1])\n"]},{"ename":"RuntimeError","evalue":"mat1 and mat2 shapes cannot be multiplied (16384x1 and 256x128)","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[1;32mIn[150], line 53\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Generate fake image batch with the generator\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(noise\u001b[38;5;241m.\u001b[39mshape,embeddings\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 53\u001b[0m fake_images \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhi after fake images\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Forward pass real batch and calculate loss\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\Yashk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\Yashk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[1;32mD:\\clg files\\RecSys\\py_files\\dcgan_model.py:40\u001b[0m, in \u001b[0;36mGenerator.forward\u001b[1;34m(self, noise, text)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, noise, text):\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# Apply text embedding to the input text\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m     text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mview(text\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], text\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Reshape to match the generator input size\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([text, noise], \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Concatenate text embedding with noise\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\Yashk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\Yashk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\Yashk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[1;32mc:\\Users\\Yashk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\Yashk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\Yashk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (16384x1 and 256x128)"]}],"source":["# training loop\n","import torch.nn.functional as F\n","\n","# iterating over number of epochs\n","from datetime import date\n","\n","start_time = time.time()\n","for epoch in range(num_epochs):\n","    \n","    batch_time = time.time()\n","    print('Epoch: {}'.format(epoch+1))\n","    #iterating over each batch\n","    for batch_idx,batch in enumerate(train_loader):   \n","\n","        print(f'epoch: {epoch+1}, batch: {batch_idx+1}/{len(train_loader)}')\n","        \n","        # reading the data into variables and moving them to device\n","        images = batch[0][0].to(device)\n","        # print('hi after images')\n","        wrong_images = batch[0][0].to(device)\n","        # print('hi after wrong images')\n","        embeddings = batch[1].to(device).float()\n","\n","        fixed_size = 256 \n","        padding = fixed_size - embeddings.size(1)\n","\n","        # Pad the tensor\n","        if padding > 0:\n","            embeddings = F.pad(embeddings, (0, padding))\n","        elif padding < 0:\n","            embeddings = embeddings[:, :fixed_size]\n","\n","        batch_size = images.size(0)\n","        print(f'embeddings size: {embeddings.size()}')\n","        # ================================================================== #\n","        #                      Train the discriminator                       #\n","        # ================================================================== #\n","        print('training the descriminator')\n","        # Clear gradients for the discriminator\n","        optimizer_D.zero_grad()\n","        \n","        # Generate random noise\n","        noise_dim = 256  \n","        batch_size = images.size(0)\n","        noise = torch.randn(batch_size, noise_dim).to(device)\n","        noise = noise.view(noise.shape[0], noise.shape[1], 1, 1)\n","\n","        # Reshape the embeddings tensor to match the noise tensor\n","        embeddings = embeddings.view(embeddings.shape[0], embeddings.shape[1], 1, 1)\n","\n","        # Generate fake image batch with the generator\n","        print(noise.shape,embeddings.shape)\n","        fake_images = generator(noise, embeddings)\n","        print('hi after fake images')\n","        # Forward pass real batch and calculate loss\n","        real_out, real_act = discriminator(images, embeddings)\n","        d_loss_real = criterion(real_out, torch.full_like(real_out, real_label, device=device))\n","        \n","        # Forward pass wrong batch and calculate loss\n","        wrong_out, wrong_act = discriminator(wrong_images, embeddings)\n","        d_loss_wrong = criterion(wrong_out, torch.full_like(wrong_out, fake_label, device=device))\n","        \n","        # Forward pass fake batch and calculate loss\n","        fake_out, fake_act = discriminator(fake_images.detach(), embeddings)\n","        d_loss_fake = criterion(fake_out, torch.full_like(fake_out, fake_label, device=device))\n","        \n","        # Compute total discriminator loss\n","        d_loss = d_loss_real + d_loss_wrong + d_loss_fake\n","        \n","        # Backpropagate the gradients\n","        d_loss.backward()\n","        \n","        # Update the discriminator\n","        optimizer_D.step()\n","        \n","        # ================================================================== #\n","        #                        Train the generator                         #\n","        # ================================================================== #\n","        print('Training the generator')\n","        # Clear gradients for the generator\n","        optimizer_G.zero_grad()\n","       \n","        # Generate new fake images using Generator\n","        fake_images = generator(noise, embeddings)\n","        \n","        # Get discriminator output for the new fake images\n","        out_fake, act_fake = discriminator(fake_images, embeddings)\n","        \n","        # Get discriminator output for the real images\n","        out_real, act_real = discriminator(images, embeddings)\n","        \n","        # Calculate losses\n","        g_bce = criterion(out_fake, torch.full_like(out_fake, real_label, device=device)) \n","        g_l1 = l1_coef * l1_loss(fake_images, images)\n","        g_l2 = l2_coef * l2_loss(torch.mean(act_fake, 0), torch.mean(act_real, 0).detach())\n","        \n","        # Compute total generator loss\n","        g_loss = g_bce + g_l1 + g_l2\n","        \n","        # Backpropagate the gradients\n","        g_loss.backward()\n","        \n","        # Update the generator\n","        optimizer_G.step()\n","        \n","        # adding loss to the list\n","        D_losses.append(d_loss.item())\n","        G_losses.append(g_loss.item())\n","        \n","        # progress based on log_interval\n","        if (batch_idx+1) % log_interval == 0 and batch_idx > 0:\n","            print('Epoch {} [{}/{}] loss_D: {:.4f} loss_G: {:.4f} time: {:.2f}'.format(\n","                          epoch+1, batch_idx+1, len(train_loader),\n","                          d_loss.mean().item(),\n","                          g_loss.mean().item(),\n","                          time.time() - batch_time))\n","        \n","        # storing generator output after every 10 epochs\n","        if batch_idx == len(train_loader)-1 and ((epoch+1)%10==0 or epoch==0):\n","            viz_sample = torch.cat((images[:32], fake_images[:32]), 0)\n","            vutils.save_image(viz_sample,\n","            os.path.join(output_save_path, 'output_{}_epoch_{}.png'.format(date,epoch+1)),\n","                              nrow=8,normalize=True)\n","\n","# saving the trained models\n","torch.save(generator.state_dict(), os.path.join(model_save_path, 'generator_{}.pth'.format(date)))\n","torch.save(discriminator.state_dict(), os.path.join(model_save_path,'discriminator_{}.pth'.format(date)))\n","        \n","print('Total train time: {:.2f}'.format(time.time() - start_time))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# generator loss plot\n","plt.figure(figsize=(10,5))\n","plt.title(\"Generator Loss During Training\")\n","plt.plot(G_losses)\n","plt.xlabel(\"iterations\")\n","plt.ylabel(\"Loss\")\n","plt.show()\n","\n","plt.savefig(os.path.join(output_save_path, 'output_generatorLoss_{}.png'.format(date)))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# discriminator loss plot\n","plt.figure(figsize=(10,5))\n","plt.title(\"Discriminator Loss During Training\")\n","plt.plot(D_losses)\n","plt.xlabel(\"iterations\")\n","plt.ylabel(\"Loss\")\n","plt.show()\n","\n","plt.savefig(os.path.join(output_save_path, 'output_discriminatorLoss_{}.png'.format(date)))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Get all file names from the \"generated_images\" directory\n","file_names = os.listdir(output_save_path)\n","file_names = [name for name in file_names if name.startswith('output_{}_'.format(date))]\n","\n","# Sort the file names numerically\n","file_names = sorted(file_names, key=lambda name: int(name.split('_')[3].split('.')[0]))\n","\n","# Create a list to store the read images\n","images = []\n","\n","for file_name in file_names:\n","    images.append(imageio.imread(os.path.join(output_save_path,file_name)))\n","\n","imageio.mimsave(os.path.join(output_save_path, 'output_gif_{}.gif'.format(date)), images, fps=1) "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from IPython.display import Image\n","\n","# Load the GIF\n","with open(os.path.join(output_save_path, 'output_gif_{}.gif'.format(date)),'rb') as file:\n","    display(Image(file.read()))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4891975,"sourceId":8245670,"sourceType":"datasetVersion"},{"datasetId":4892017,"sourceId":8245726,"sourceType":"datasetVersion"},{"datasetId":4908067,"sourceId":8267485,"sourceType":"datasetVersion"},{"datasetId":4908285,"sourceId":8267780,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":4}
