{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T04:19:54.714342Z","iopub.status.busy":"2024-04-30T04:19:54.713732Z","iopub.status.idle":"2024-04-30T04:20:07.474835Z","shell.execute_reply":"2024-04-30T04:20:07.473686Z","shell.execute_reply.started":"2024-04-30T04:19:54.714303Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pytorch-pretrained-bert in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.6.2)\n","Requirement already satisfied: torch>=0.4.1 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytorch-pretrained-bert) (2.3.0)\n","Requirement already satisfied: numpy in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytorch-pretrained-bert) (1.26.4)\n","Requirement already satisfied: boto3 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytorch-pretrained-bert) (1.34.94)\n","Requirement already satisfied: requests in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytorch-pretrained-bert) (2.31.0)\n","Requirement already satisfied: tqdm in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytorch-pretrained-bert) (4.66.2)\n","Requirement already satisfied: regex in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytorch-pretrained-bert) (2024.4.28)\n","Requirement already satisfied: filelock in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.14.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=0.4.1->pytorch-pretrained-bert) (4.11.0)\n","Requirement already satisfied: sympy in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=0.4.1->pytorch-pretrained-bert) (1.12)\n","Requirement already satisfied: networkx in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.3)\n","Requirement already satisfied: jinja2 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.1.3)\n","Requirement already satisfied: fsspec in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=0.4.1->pytorch-pretrained-bert) (2024.3.1)\n","Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=0.4.1->pytorch-pretrained-bert) (2021.4.0)\n","Requirement already satisfied: botocore<1.35.0,>=1.34.94 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from boto3->pytorch-pretrained-bert) (1.34.94)\n","Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from boto3->pytorch-pretrained-bert) (1.0.1)\n","Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from boto3->pytorch-pretrained-bert) (0.10.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->pytorch-pretrained-bert) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->pytorch-pretrained-bert) (2.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->pytorch-pretrained-bert) (2.2.1)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->pytorch-pretrained-bert) (2024.2.2)\n","Requirement already satisfied: colorama in c:\\users\\yashk\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->pytorch-pretrained-bert) (0.4.6)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\yashk\\appdata\\roaming\\python\\python312\\site-packages (from botocore<1.35.0,>=1.34.94->boto3->pytorch-pretrained-bert) (2.9.0.post0)\n","Requirement already satisfied: intel-openmp==2021.* in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=0.4.1->pytorch-pretrained-bert) (2021.4.0)\n","Requirement already satisfied: tbb==2021.* in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=0.4.1->pytorch-pretrained-bert) (2021.12.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch>=0.4.1->pytorch-pretrained-bert) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch>=0.4.1->pytorch-pretrained-bert) (1.3.0)\n","Requirement already satisfied: six>=1.5 in c:\\users\\yashk\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.94->boto3->pytorch-pretrained-bert) (1.16.0)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install pytorch-pretrained-bert"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T04:20:07.491627Z","iopub.status.busy":"2024-04-30T04:20:07.491322Z","iopub.status.idle":"2024-04-30T04:21:12.769506Z","shell.execute_reply":"2024-04-30T04:21:12.768369Z","shell.execute_reply.started":"2024-04-30T04:20:07.491601Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: tensorflow in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.16.1)\n","Requirement already satisfied: tensorflow-intel==2.16.1 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (2.16.1)\n","Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.1.0)\n","Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (24.3.25)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.5.4)\n","Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.2.0)\n","Requirement already satisfied: h5py>=3.10.0 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.11.0)\n","Requirement already satisfied: libclang>=13.0.0 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (18.1.1)\n","Requirement already satisfied: ml-dtypes~=0.3.1 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.3.2)\n","Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.3.0)\n","Requirement already satisfied: packaging in c:\\users\\yashk\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (24.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.25.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.31.0)\n","Requirement already satisfied: setuptools in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (69.5.1)\n","Requirement already satisfied: six>=1.12.0 in c:\\users\\yashk\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.11.0)\n","Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.62.2)\n","Requirement already satisfied: tensorboard<2.17,>=2.16 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.16.2)\n","Requirement already satisfied: keras>=3.0.0 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.3.3)\n","Requirement already satisfied: numpy<2.0.0,>=1.26.0 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.26.4)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow) (0.43.0)\n","Requirement already satisfied: rich in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (13.7.1)\n","Requirement already satisfied: namex in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.0.8)\n","Requirement already satisfied: optree in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.11.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2.2.1)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2024.2.2)\n","Requirement already satisfied: markdown>=2.6.8 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.6)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.0.2)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.1.5)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\yashk\\appdata\\roaming\\python\\python312\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (2.17.2)\n","Requirement already satisfied: mdurl~=0.1 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.1.2)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install --upgrade tensorflow"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T04:21:12.850591Z","iopub.status.busy":"2024-04-30T04:21:12.850285Z","iopub.status.idle":"2024-04-30T04:22:00.871982Z","shell.execute_reply":"2024-04-30T04:22:00.870832Z","shell.execute_reply.started":"2024-04-30T04:21:12.850564Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Note: you may need to restart the kernel to use updated packages.\n"]},{"name":"stderr","output_type":"stream","text":["The system cannot find the file specified.\n"]}],"source":["%pip install 'keras<3.0.0' mediapipe-model-maker"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T04:22:00.874102Z","iopub.status.busy":"2024-04-30T04:22:00.873754Z","iopub.status.idle":"2024-04-30T04:22:10.689661Z","shell.execute_reply":"2024-04-30T04:22:10.688906Z","shell.execute_reply.started":"2024-04-30T04:22:00.874051Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\Yashk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mvutils\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtransforms\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\Yashk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\__init__.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodulefinder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Module\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n","File \u001b[1;32mc:\\Users\\Yashk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malexnet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvnext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdensenet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mefficientnet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n","File \u001b[1;32mc:\\Users\\Yashk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\convnext.py:8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn, Tensor\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m functional \u001b[38;5;28;01mas\u001b[39;00m F\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmisc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Conv2dNormActivation, Permute\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstochastic_depth\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StochasticDepth\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_presets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageClassification\n","File \u001b[1;32mc:\\Users\\Yashk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\ops\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_register_onnx_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _register_custom_op\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mboxes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      3\u001b[0m     batched_nms,\n\u001b[0;32m      4\u001b[0m     box_area,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m     remove_small_boxes,\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mciou_loss\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m complete_box_iou_loss\n","File \u001b[1;32mc:\\Users\\Yashk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\ops\\_register_onnx_ops.py:5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01monnx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m symbolic_opset11 \u001b[38;5;28;01mas\u001b[39;00m opset11\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01monnx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msymbolic_helper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse_args\n\u001b[0;32m      8\u001b[0m _ONNX_OPSET_VERSION_11 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m11\u001b[39m\n","File \u001b[1;32mc:\\Users\\Yashk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\onnx\\__init__.py:46\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CheckerError  \u001b[38;5;66;03m# Backwards compatibility\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     35\u001b[0m     _optimize_graph,\n\u001b[0;32m     36\u001b[0m     _run_symbolic_function,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     43\u001b[0m     unregister_custom_op_symbolic,\n\u001b[0;32m     44\u001b[0m )\n\u001b[1;32m---> 46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_internal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexporter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# usort:skip. needs to be last to avoid circular import\u001b[39;00m\n\u001b[0;32m     47\u001b[0m     DiagnosticOptions,\n\u001b[0;32m     48\u001b[0m     ExportOptions,\n\u001b[0;32m     49\u001b[0m     ONNXProgram,\n\u001b[0;32m     50\u001b[0m     ONNXProgramSerializer,\n\u001b[0;32m     51\u001b[0m     ONNXRuntimeOptions,\n\u001b[0;32m     52\u001b[0m     InvalidExportOptionsError,\n\u001b[0;32m     53\u001b[0m     OnnxExporterError,\n\u001b[0;32m     54\u001b[0m     OnnxRegistry,\n\u001b[0;32m     55\u001b[0m     dynamo_export,\n\u001b[0;32m     56\u001b[0m     enable_fake_mode,\n\u001b[0;32m     57\u001b[0m )\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_internal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01monnxruntime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     60\u001b[0m     is_onnxrt_backend_supported,\n\u001b[0;32m     61\u001b[0m     OrtBackend \u001b[38;5;28;01mas\u001b[39;00m _OrtBackend,\n\u001b[0;32m     62\u001b[0m     OrtBackendOptions \u001b[38;5;28;01mas\u001b[39;00m _OrtBackendOptions,\n\u001b[0;32m     63\u001b[0m     OrtExecutionProvider \u001b[38;5;28;01mas\u001b[39;00m _OrtExecutionProvider,\n\u001b[0;32m     64\u001b[0m )\n\u001b[0;32m     66\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# Modules\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msymbolic_helper\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_onnxrt_backend_supported\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    115\u001b[0m ]\n","File \u001b[1;32mc:\\Users\\Yashk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\onnx\\_internal\\exporter.py:44\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01monnx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_internal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _beartype, io_adapter\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01monnx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_internal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdiagnostics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m infra\n\u001b[1;32m---> 44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01monnx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_internal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     45\u001b[0m     decomposition_table,\n\u001b[0;32m     46\u001b[0m     patcher \u001b[38;5;28;01mas\u001b[39;00m patcher,\n\u001b[0;32m     47\u001b[0m     registration,\n\u001b[0;32m     48\u001b[0m     serialization \u001b[38;5;28;01mas\u001b[39;00m fx_serialization,\n\u001b[0;32m     49\u001b[0m )\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# We can only import onnx from this module in a type-checking context to ensure that\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# 'import torch.onnx' continues to work without having 'onnx' installed. We fully\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# 'import onnx' inside of dynamo_export (by way of _assert_dependencies).\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n","File \u001b[1;32mc:\\Users\\Yashk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\onnx\\_internal\\fx\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpatcher\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ONNXTorchPatcher\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mserialization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m save_model_with_external_data\n\u001b[0;32m      5\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msave_model_with_external_data\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mONNXTorchPatcher\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      8\u001b[0m ]\n","File \u001b[1;32mc:\\Users\\Yashk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\onnx\\_internal\\fx\\patcher.py:11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# safetensors is not an exporter requirement, but needed for some huggingface models\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msafetensors\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[import]  # noqa: F401\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[import]\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msafetensors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m torch \u001b[38;5;28;01mas\u001b[39;00m safetensors_torch  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     has_safetensors_and_transformers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\Yashk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\__init__.py:26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     28\u001b[0m     OptionalDependencyNotAvailable,\n\u001b[0;32m     29\u001b[0m     _LazyModule,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m     logging,\n\u001b[0;32m     49\u001b[0m )\n\u001b[0;32m     52\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)  \u001b[38;5;66;03m# pylint: disable=invalid-name\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\Yashk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\dependency_versions_check.py:16\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdependency_versions_table\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deps\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m require_version, require_version_core\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# define which module versions we always want to check at run time\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# (usually the ones defined in `install_requires` in setup.py)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# order specific notes:\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# - tqdm must be checked before tokenizers\u001b[39;00m\n\u001b[0;32m     25\u001b[0m pkgs_to_check_at_runtime \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtqdm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyyaml\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     38\u001b[0m ]\n","File \u001b[1;32mc:\\Users\\Yashk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\__init__.py:18\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#!/usr/bin/env python\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# coding=utf-8\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_full_repo_name  \u001b[38;5;66;03m# for backward compatibility\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HF_HUB_DISABLE_TELEMETRY \u001b[38;5;28;01mas\u001b[39;00m DISABLE_TELEMETRY  \u001b[38;5;66;03m# for backward compatibility\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m version\n","File \u001b[1;32m<frozen importlib._bootstrap>:1412\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n","File \u001b[1;32mc:\\Users\\Yashk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\__init__.py:503\u001b[0m, in \u001b[0;36m_attach.<locals>.__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    501\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m attr_to_modules:\n\u001b[0;32m    502\u001b[0m     submod_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpackage_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr_to_modules[name]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 503\u001b[0m     submod \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubmod_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    504\u001b[0m     attr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(submod, name)\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;66;03m# If the attribute lives in a file (module) with the same\u001b[39;00m\n\u001b[0;32m    507\u001b[0m     \u001b[38;5;66;03m# name as the attribute, ensure that the attribute and *not*\u001b[39;00m\n\u001b[0;32m    508\u001b[0m     \u001b[38;5;66;03m# the module is accessible on the package.\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\Yashk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\importlib\\__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\Yashk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\hf_api.py:50\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm \u001b[38;5;28;01mas\u001b[39;00m base_tqdm\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontrib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconcurrent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m thread_map\n\u001b[1;32m---> 50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_commit_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     51\u001b[0m     CommitOperation,\n\u001b[0;32m     52\u001b[0m     CommitOperationAdd,\n\u001b[0;32m     53\u001b[0m     CommitOperationCopy,\n\u001b[0;32m     54\u001b[0m     CommitOperationDelete,\n\u001b[0;32m     55\u001b[0m     _fetch_files_to_copy,\n\u001b[0;32m     56\u001b[0m     _fetch_upload_modes,\n\u001b[0;32m     57\u001b[0m     _prepare_commit_payload,\n\u001b[0;32m     58\u001b[0m     _upload_lfs_files,\n\u001b[0;32m     59\u001b[0m     _warn_on_overwriting_operations,\n\u001b[0;32m     60\u001b[0m )\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_inference_endpoints\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InferenceEndpoint, InferenceEndpointType\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_multi_commits\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     63\u001b[0m     MULTI_COMMIT_PR_CLOSE_COMMENT_FAILURE_BAD_REQUEST_TEMPLATE,\n\u001b[0;32m     64\u001b[0m     MULTI_COMMIT_PR_CLOSE_COMMENT_FAILURE_NO_CHANGES_TEMPLATE,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     73\u001b[0m     plan_multi_commits,\n\u001b[0;32m     74\u001b[0m )\n","File \u001b[1;32mc:\\Users\\Yashk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\_commit_api.py:19\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontrib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconcurrent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m thread_map\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ENDPOINT, HF_HUB_ENABLE_HF_TRANSFER\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfile_download\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hf_hub_url\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlfs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UploadInfo, lfs_upload, post_lfs_batch_info\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     22\u001b[0m     FORBIDDEN_FOLDERS,\n\u001b[0;32m     23\u001b[0m     EntryNotFoundError,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m     validate_hf_hub_args,\n\u001b[0;32m     30\u001b[0m )\n","File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n","File \u001b[1;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n","File \u001b[1;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n","File \u001b[1;32m<frozen importlib._bootstrap_external>:991\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n","File \u001b[1;32m<frozen importlib._bootstrap_external>:1087\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n","File \u001b[1;32m<frozen importlib._bootstrap_external>:1186\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import torch\n","import torch.nn as nn\n","import torchvision.utils as vutils\n","import torchvision.transforms as transforms\n","import os\n","import sys\n","input_dir = r'D:\\clg files\\RecSys\\py_files'\n","sys.path.append(input_dir)\n","\n","from data_util import AttDesDataset\n","from utils import weights_init\n","\n","# from dcgan_model import Generator, Discriminator\n","import time\n","import imageio\n","import matplotlib.pyplot as plt\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T04:22:10.691416Z","iopub.status.busy":"2024-04-30T04:22:10.690874Z","iopub.status.idle":"2024-04-30T04:22:10.743811Z","shell.execute_reply":"2024-04-30T04:22:10.742709Z","shell.execute_reply.started":"2024-04-30T04:22:10.691388Z"},"trusted":true},"outputs":[],"source":["# Setting device to cuda\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(\"Using Device\", device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T04:22:10.746034Z","iopub.status.busy":"2024-04-30T04:22:10.745559Z","iopub.status.idle":"2024-04-30T04:22:10.762543Z","shell.execute_reply":"2024-04-30T04:22:10.761792Z","shell.execute_reply.started":"2024-04-30T04:22:10.745986Z"},"trusted":true},"outputs":[],"source":["# directory to store output images\n","output_save_path = './generated_images/'\n","os.makedirs(output_save_path, exist_ok=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T04:22:10.764090Z","iopub.status.busy":"2024-04-30T04:22:10.763759Z","iopub.status.idle":"2024-04-30T04:22:10.770347Z","shell.execute_reply":"2024-04-30T04:22:10.769432Z","shell.execute_reply.started":"2024-04-30T04:22:10.764063Z"},"trusted":true},"outputs":[],"source":["# directory to store trained models\n","model_save_path = './saved_models/'\n","os.makedirs(model_save_path, exist_ok=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torch.utils.data import Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torch.nn.utils.rnn import pad_sequence\n","\n","class CustomDataset(Dataset):\n","    def __init__(self, data):\n","        self.images = data['images']\n","        self.captions = data['captions']    \n","        self.wrong_images = data['wrong_images']\n","\n","    def __getitem__(self, index):\n","        image = self.images[index]\n","        caption = self.captions[index]\n","        caption_tensor = torch.tensor(caption)\n","        wrong_images = self.wrong_images[index] \n","        return image, caption_tensor, wrong_images\n","\n","    def __len__(self):\n","        return len(self.images)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def collate_fn(batch):\n","    images = [item[0] for item in batch]\n","    captions = [item[1] for item in batch]\n","\n","    # Stack images\n","    images = torch.stack(images, 0)\n","\n","    # Truncate or pad captions\n","    max_length = 128  # whatever fixed length\n","    captions = [cap[:max_length] for cap in captions]  # Truncate\n","    embedding_size = 768 \n","    captions = [torch.cat([cap, torch.zeros(max_length - cap.size(0), embedding_size)]) for cap in captions]\n","\n","    # Stack captions\n","    captions = torch.stack(captions, 0)\n","\n","    return images, captions"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from PIL import Image\n","\n","directory = r'D:\\clg files\\RecSys\\data_files\\wrong_images'\n","normalize = transforms.Normalize(mean=[0, 0, 0], std=[1, 1, 1])\n","transform = transforms.Compose([\n","                                            transforms.Resize((448,448)),\n","                                            transforms.RandomHorizontalFlip(),\n","                                            transforms.ToTensor(),\n","                                            normalize,\n","                                        ])\n","print(directory)\n","wrong_imgs = []\n","for filename in os.listdir(directory):\n","        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n","                full_path = os.path.join(directory, filename)\n","                img = Image.open(full_path)\n","                img = transform(img)\n","                wrong_imgs.append(img)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(len(wrong_imgs))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["wrong_imgs[0].shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T04:22:10.775506Z","iopub.status.busy":"2024-04-30T04:22:10.775153Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import DataLoader, SubsetRandomSampler\n","import numpy as np\n","\n","data_root = r'data_files\\smoll.csv'\n","split_root = ''\n","dataset_name = 'Furniture'\n","normalize = transforms.Normalize(mean=[0, 0, 0], std=[1, 1, 1])\n","batch_size = 4 #128\n","dataset = AttDesDataset(data_root, dataset_name, transform=transforms.Compose([\n","                                            transforms.Resize((448,448)),\n","                                            transforms.RandomHorizontalFlip(),\n","                                            transforms.ToTensor(),\n","                                            normalize,\n","                                        ]))\n","\n","temp_dataset = {'images': dataset.images, 'captions': dataset.descriptions, 'wrong_images': wrong_imgs}\n","custom_dataset = CustomDataset(temp_dataset)\n","print(len(custom_dataset))\n","# print(len(custom_dataset['captions']))\n","num_samples = len(custom_dataset)\n","indices = list(range(num_samples))\n","split = int(np.floor(0.8 * num_samples))  # 80-20 train-validation split\n","\n","# Randomly shuffle the indices\n","np.random.shuffle(indices)\n","\n","# Split the indices into train and validation sets\n","train_indices, val_indices = indices[:split], indices[split:]\n","\n","# Define samplers for train and validation sets\n","train_sampler = SubsetRandomSampler(train_indices)\n","val_sampler = SubsetRandomSampler(val_indices)\n","\n","# Define DataLoader for train and validation sets using samplers\n","train_loader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n","val_loader = DataLoader(custom_dataset, batch_size=batch_size, sampler=val_sampler, num_workers=0)\n","\n","# Check the number of batches in train and validation loaders\n","print(\"No of batches in train loader: \", len(train_loader))\n","print(\"No of batches in validation loader: \", len(val_loader))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(len(wrong_imgs))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["len(dataset.images)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["len(dataset.descriptions)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dataset.images[0].shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["len(dataset.descriptions[0])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(dataset.descriptions[0])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(len(temp_dataset['captions'][0]))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["len(custom_dataset[1][1])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# for i, data in enumerate(train_loader):\n","#     print(type(data))  # This will print the type of the data element\n","#     if isinstance(data, list) or isinstance(data, tuple):\n","#         print(len(data))  \n","#     elif isinstance(data, dict):\n","#         print(data.keys())\n","#     if i >= 0: \n","#         break"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["prev1 = 448\n","\n","for batch_idx,batch in enumerate(train_loader):\n","    print(batch_idx)\n","    print(type(batch))\n","    print(batch[0].shape)\n","    print(batch[1].shape)\n","    if batch_idx == 0:\n","        break   \n","    # if(batch[0].shape[2] != prev1 and batch[3].shape[0] != prev1):\n","    #     print(\"Batch size mismatch\")\n","    #     print(f'batch_idx: {batch_idx}, batch[0].shape: {batch[0].shape}, batch[1].shape: {batch[1].shape}')\n","    #     break"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(len(custom_dataset.captions[0]))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# setting up parameters\n","noise_dim = 100\n","embed_dim = 768\n","embed_out_dim = 100\n","batch_size = 4 #128\n","real_label = 1\n","fake_label = 0\n","learning_rate = 0.0002\n","l1_coef = 50\n","l2_coef = 100\n","\n","num_epochs = 25\n","log_interval = 18 #43"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# loss functions\n","criterion = nn.BCELoss()\n","l2_loss = nn.MSELoss()\n","l1_loss = nn.L1Loss()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# lists to store losses\n","D_losses = []\n","G_losses = []"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# initializing generator\n","from dcgan1 import Generator, Discriminator\n","\n","generator = Generator(channels=3, embed_dim=embed_dim, noise_dim=noise_dim, embed_out_dim=embed_out_dim).to(device)\n","generator.apply(weights_init)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# initializing discriminator\n","discriminator = Discriminator(channels=3, embed_dim=embed_dim, embed_out_dim=embed_out_dim).to(device)\n","discriminator.apply(weights_init)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# setting up Adam optimizer for Generator and Discriminator\n","optimizer_G = torch.optim.Adam(generator.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n","optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=learning_rate, betas=(0.5, 0.999))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# training loop\n","import torch.nn.functional as F\n","import PIL\n","# iterating over number of epochs\n","from datetime import date\n","from datetime import timedelta\n","from datetime import datetime\n","\n","start_time = time.time()\n","for epoch in range(num_epochs):\n","    \n","    batch_time = time.time()\n","    print('Epoch: {}'.format(epoch+1))\n","    #iterating over each batch\n","    for batch_idx,batch in enumerate(train_loader):   \n","\n","        print(f'epoch: {epoch+1}, batch: {batch_idx+1}/{len(train_loader)}')\n","        # print(batch_idx)\n","        \n","        # reading the data into variables and moving them to device\n","        images = batch[0].to(device)\n","        # print('hi after images')\n","        wrong_images = batch[2].to(device)\n","        # print('hi after wrong images')\n","        embeddings = batch[1].to(device).float()\n","\n","        batch_size = images.size(0)\n","        # print(f'embeddings size: {embeddings.size()}')\n","        # ================================================================== #\n","        #                      Train the discriminator                       #\n","        # ================================================================== #\n","        # print('training the descriminator')\n","        # Clear gradients for the discriminator\n","        optimizer_D.zero_grad()\n","        \n","        # Generate random noise\n","        noise_dim = 100  \n","        batch_size = images.size(0)\n","        noise = torch.randn(embeddings.size(0), noise_dim).to(device)\n","        # noise = noise.view(embeddings.size(0), noise_dim, 1, 1)\n","   \n","        # Generate fake image batch with the generator\n","        # print(f'noise size: {noise.size()}')\n","        # print(f'embeddings size: {embeddings.size()}')\n","        \n","        fake_images = generator(noise, embeddings)\n","        # print('hi after fake images')\n","        # if batch_idx == 0:\n","            # print(f'noise size: {noise.size()}')\n","            # print(f'embeddings size: {embeddings.size()}')\n","            # print(f'fake_images size: {fake_images.size()}')\n","        fake_images = F.interpolate(fake_images, size=(448, 448), mode='bilinear', align_corners=False)\n","        # print('fake images after interpolation: ', fake_images.size())\n","        # Forward pass real batch and calculate loss\n","\n","        real_out, real_act = discriminator(images, embeddings)\n","        d_loss_real = criterion(real_out, torch.full_like(real_out, real_label, device=device))\n","        \n","        # Forward pass wrong batch and calculate loss\n","        wrong_out, wrong_act = discriminator(wrong_images, embeddings)\n","        d_loss_wrong = criterion(wrong_out, torch.full_like(wrong_out, fake_label, device=device))\n","        \n","        # Forward pass fake batch and calculate loss\n","        fake_out, fake_act = discriminator(fake_images.detach(), embeddings)\n","        d_loss_fake = criterion(fake_out, torch.full_like(fake_out, fake_label, device=device))\n","        \n","        # Compute total discriminator loss\n","        d_loss = d_loss_real + d_loss_wrong + d_loss_fake\n","        \n","        # Backpropagate the gradients\n","        d_loss.backward()\n","        \n","        # Update the discriminator\n","        optimizer_D.step()\n","        \n","        # ================================================================== #\n","        #                        Train the generator                         #\n","        # ================================================================== #\n","        # print('Training the generator')\n","        # Clear gradients for the generator\n","        optimizer_G.zero_grad()\n","       \n","        # Generate new fake images using Generator\n","        fake_images = generator(noise, embeddings)\n","        # print(f'fake_images size: {fake_images.size()}')\n","        fake_images = F.interpolate(fake_images, size=(448, 448), mode='bilinear', align_corners=False)\n","        # print('fake images after interpolation: ', fake_images.size())\n","        # Get discriminator output for the new fake images\n","        out_fake, act_fake = discriminator(fake_images, embeddings)\n","        # Get discriminator output for the real images\n","        out_real, act_real = discriminator(images, embeddings)\n","        \n","        # Calculate losses\n","        g_bce = criterion(out_fake, torch.full_like(out_fake, real_label, device=device)) \n","        g_l1 = l1_coef * l1_loss(fake_images, images)\n","        g_l2 = l2_coef * l2_loss(torch.mean(act_fake, 0), torch.mean(act_real, 0).detach())\n","        \n","        # Compute total generator loss\n","        g_loss = g_bce + g_l1 + g_l2\n","        \n","        # Backpropagate the gradients\n","        g_loss.backward()\n","        \n","        # Update the generator\n","        optimizer_G.step()\n","        \n","        # adding loss to the list\n","        D_losses.append(d_loss.item())\n","        G_losses.append(g_loss.item())\n","        \n","        # progress based on log_interval\n","        if (batch_idx+1) % log_interval == 0 and batch_idx > 0:\n","            print('Epoch {} [{}/{}] loss_D: {:.4f} loss_G: {:.4f} time: {:.2f}'.format(\n","                          epoch+1, batch_idx+1, len(train_loader),\n","                          d_loss.mean().item(),\n","                          g_loss.mean().item(),\n","                          time.time() - batch_time))\n","        \n","        # storing generator output after every 10 epochs\n","        if batch_idx == len(train_loader)-1 and ((epoch+1)%5==0 or epoch==0):\n","            viz_sample = torch.cat((images[:32], fake_images[:32]), 0)\n","            current_time = datetime.now().strftime(\"%H_%M_%S\")\n","            vutils.save_image(viz_sample,\n","            os.path.join(output_save_path, 'output_{}_epoch_{}_time_{}.png'.format(date.today(),epoch+1, current_time)),\n","                            nrow=8,normalize=True)\n","            # viz_sample.show()\n","\n","\n","total_time = time.time() - start_time\n","formatted_time = str(timedelta(seconds=total_time))\n","\n","print('Total train time: {}'.format(formatted_time))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# saving the trained models\n","torch.save(generator.state_dict(), os.path.join(model_save_path, 'generator_{}.pth'.format(date.today(),epoch+1)))\n","torch.save(discriminator.state_dict(), os.path.join(model_save_path,'discriminator_{}.pth'.format(date.today(),epoch+1)))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# generator loss plot\n","plt.figure(figsize=(10,5))\n","plt.title(\"Generator Loss During Training\")\n","plt.plot(G_losses)\n","plt.xlabel(\"iterations\")\n","plt.ylabel(\"Loss\")\n","plt.show()\n","\n","# plt.savefig(os.path.join(output_save_path, 'output_generatorLoss_{}.png'.format(date.today())))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# discriminator loss plot\n","plt.figure(figsize=(10,5))\n","plt.title(\"Discriminator Loss During Training\")\n","plt.plot(D_losses)\n","plt.xlabel(\"iterations\")\n","plt.ylabel(\"Loss\")\n","plt.show()\n","\n","# plt.savefig(os.path.join(output_save_path, 'output_discriminatorLoss_{}.png'.format(date.today())))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torchvision.utils as vutils\n","import os\n","from datetime import date\n","\n","# Assuming output_save_path and epoch are defined\n","output_save_path = './'\n","epoch = 1\n","\n","vutils.save_image(viz_sample, os.path.join(output_save_path, 'output_{}_epoch_{}.png'.format(date.today(), epoch+1)), normalize=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["viz_sample = torch.cat((images[:32], fake_images[:32]), 0)\n","vutils.save_image(viz_sample,\n","                  os.path.join(output_save_path, 'output_{}_epoch_{}.png'.format(date.today(),epoch+1)),\n","                              nrow=8,normalize=True)\n","# viz_sample.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from PIL import Image\n","import torchvision.transforms as transforms\n","\n","# Assuming viz_sample is your tensor\n","# First, detach and move the tensor to cpu\n","viz_sample = viz_sample.detach().cpu()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Convert the tensor to a PIL Image\n","first_image = viz_sample[0]\n","viz_sample_pil = transforms.ToPILImage()(first_image)\n","\n","# Display the image\n","viz_sample_pil.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Convert the tensor to a PIL Image\n","first_image = viz_sample[1]\n","viz_sample_pil = transforms.ToPILImage()(first_image)\n","\n","# Display the image\n","viz_sample_pil.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Convert the tensor to a PIL Image\n","first_image = viz_sample[2]\n","viz_sample_pil = transforms.ToPILImage()(first_image)\n","\n","# Display the image\n","viz_sample_pil.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Convert the tensor to a PIL Image\n","first_image = viz_sample[3]\n","viz_sample_pil = transforms.ToPILImage()(first_image)\n","\n","# Display the image\n","viz_sample_pil.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from pytorch_pretrained_bert.tokenization import BertTokenizer\n","from pytorch_pretrained_bert.modeling import BertModel\n","\n","def get_embeddings(text):\n","    tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n","    bert_model = BertModel.from_pretrained('bert-base-chinese')\n","    des_len = 100\n","    tokens = [\"[CLS]\"]\n","    token_obj = tokenizer.tokenize(text)\n","    tokens.extend(token_obj)\n","    tokens.append(\"[SEP]\")\n","    tokens = tokenizer.convert_tokens_to_ids(tokens)\n","\n","        # Pad or truncate to the desired length\n","    if len(tokens) < des_len:\n","        tokens += [0] * (des_len - len(tokens))  # Pad with zeros\n","    else:\n","        tokens = tokens[:des_len]  # Truncate to the maximum length\n","    \n","\n","    input_ids = tokens\n","    input_ids = torch.tensor(input_ids).unsqueeze(0)\n","\n","     # Get BERT outputs\n","    with torch.no_grad():\n","                \n","        input_ids = torch.tensor(input_ids)\n","        # print(type(input_ids))\n","        outputs = bert_model(input_ids)\n","            \n","    output_tensor = outputs[0]\n","    # Get the vector of the [CLS] token (first token)\n","    cls_vector = output_tensor[0][0][:]\n","    return cls_vector\n"," "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","def generate_image(caption):\n","    # Convert caption to embedding (if needed) and generate noise vector\n","    # You need to define how to convert the caption to an embedding\n","    # For example, you can use a pre-trained language model like BERT to get embeddings\n","    \n","    # Generate fake image using Generator\n","    with torch.no_grad():\n","        # Assuming you have a function to convert caption to embeddings\n","        embeddings = get_embeddings(caption)\n","        print(f'embeddings size: {embeddings.size()}')\n","        noise = torch.randn(batch_size, embeddings.size(0))\n","        print(f'noise size: {noise.size()}')\n","        embeddings = embeddings.unsqueeze(0)\n","        embeddings = embeddings.repeat(batch_size, 1, 1)\n","        fake_image = generator(noise, embeddings)\n","        fake_image = F.interpolate(fake_image, size=(448, 448), mode='bilinear', align_corners=False)\n","    \n","    return fake_image"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from translate import Translator\n","\n","def translate_to_chinese(text):\n","    translator = Translator(to_lang=\"zh\")\n","    translation = translator.translate(text)\n","    return translation\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["caption = \"A black room with no furniture\"\n","chinese_caption = translate_to_chinese(caption)\n","print(f'chinese_caption: {chinese_caption}')\n","generated_image = generate_image(chinese_caption)   \n","transforms.ToPILImage()(generated_image[1]).show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4891975,"sourceId":8245670,"sourceType":"datasetVersion"},{"datasetId":4892017,"sourceId":8245726,"sourceType":"datasetVersion"},{"datasetId":4908067,"sourceId":8267485,"sourceType":"datasetVersion"},{"datasetId":4908285,"sourceId":8267780,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":4}
