{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T04:19:54.714342Z","iopub.status.busy":"2024-04-30T04:19:54.713732Z","iopub.status.idle":"2024-04-30T04:20:07.474835Z","shell.execute_reply":"2024-04-30T04:20:07.473686Z","shell.execute_reply.started":"2024-04-30T04:19:54.714303Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pytorch-pretrained-bert in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.6.2)\n","Requirement already satisfied: torch>=0.4.1 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytorch-pretrained-bert) (2.3.0)\n","Requirement already satisfied: numpy in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytorch-pretrained-bert) (1.26.4)\n","Requirement already satisfied: boto3 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytorch-pretrained-bert) (1.34.94)\n","Requirement already satisfied: requests in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytorch-pretrained-bert) (2.31.0)\n","Requirement already satisfied: tqdm in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytorch-pretrained-bert) (4.66.2)\n","Requirement already satisfied: regex in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytorch-pretrained-bert) (2024.4.28)\n","Requirement already satisfied: filelock in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.14.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=0.4.1->pytorch-pretrained-bert) (4.11.0)\n","Requirement already satisfied: sympy in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=0.4.1->pytorch-pretrained-bert) (1.12)\n","Requirement already satisfied: networkx in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.3)\n","Requirement already satisfied: jinja2 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.1.3)\n","Requirement already satisfied: fsspec in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=0.4.1->pytorch-pretrained-bert) (2024.3.1)\n","Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=0.4.1->pytorch-pretrained-bert) (2021.4.0)\n","Requirement already satisfied: botocore<1.35.0,>=1.34.94 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from boto3->pytorch-pretrained-bert) (1.34.94)\n","Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from boto3->pytorch-pretrained-bert) (1.0.1)\n","Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from boto3->pytorch-pretrained-bert) (0.10.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->pytorch-pretrained-bert) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->pytorch-pretrained-bert) (2.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->pytorch-pretrained-bert) (2.2.1)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->pytorch-pretrained-bert) (2024.2.2)\n","Requirement already satisfied: colorama in c:\\users\\yashk\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->pytorch-pretrained-bert) (0.4.6)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\yashk\\appdata\\roaming\\python\\python312\\site-packages (from botocore<1.35.0,>=1.34.94->boto3->pytorch-pretrained-bert) (2.9.0.post0)\n","Requirement already satisfied: intel-openmp==2021.* in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=0.4.1->pytorch-pretrained-bert) (2021.4.0)\n","Requirement already satisfied: tbb==2021.* in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=0.4.1->pytorch-pretrained-bert) (2021.12.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch>=0.4.1->pytorch-pretrained-bert) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch>=0.4.1->pytorch-pretrained-bert) (1.3.0)\n","Requirement already satisfied: six>=1.5 in c:\\users\\yashk\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.94->boto3->pytorch-pretrained-bert) (1.16.0)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install pytorch-pretrained-bert"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T04:20:07.491627Z","iopub.status.busy":"2024-04-30T04:20:07.491322Z","iopub.status.idle":"2024-04-30T04:21:12.769506Z","shell.execute_reply":"2024-04-30T04:21:12.768369Z","shell.execute_reply.started":"2024-04-30T04:20:07.491601Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: tensorflow in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.16.1)\n","Requirement already satisfied: tensorflow-intel==2.16.1 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (2.16.1)\n","Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.1.0)\n","Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (24.3.25)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.5.4)\n","Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.2.0)\n","Requirement already satisfied: h5py>=3.10.0 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.11.0)\n","Requirement already satisfied: libclang>=13.0.0 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (18.1.1)\n","Requirement already satisfied: ml-dtypes~=0.3.1 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.3.2)\n","Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.3.0)\n","Requirement already satisfied: packaging in c:\\users\\yashk\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (24.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.25.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.31.0)\n","Requirement already satisfied: setuptools in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (69.5.1)\n","Requirement already satisfied: six>=1.12.0 in c:\\users\\yashk\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.11.0)\n","Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.62.2)\n","Requirement already satisfied: tensorboard<2.17,>=2.16 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.16.2)\n","Requirement already satisfied: keras>=3.0.0 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.3.3)\n","Requirement already satisfied: numpy<2.0.0,>=1.26.0 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.26.4)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow) (0.43.0)\n","Requirement already satisfied: rich in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (13.7.1)\n","Requirement already satisfied: namex in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.0.8)\n","Requirement already satisfied: optree in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.11.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2.2.1)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2024.2.2)\n","Requirement already satisfied: markdown>=2.6.8 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.6)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.0.2)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.1.5)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\yashk\\appdata\\roaming\\python\\python312\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (2.17.2)\n","Requirement already satisfied: mdurl~=0.1 in c:\\users\\yashk\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.1.2)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install --upgrade tensorflow"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T04:21:12.850591Z","iopub.status.busy":"2024-04-30T04:21:12.850285Z","iopub.status.idle":"2024-04-30T04:22:00.871982Z","shell.execute_reply":"2024-04-30T04:22:00.870832Z","shell.execute_reply.started":"2024-04-30T04:21:12.850564Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Note: you may need to restart the kernel to use updated packages.\n"]},{"name":"stderr","output_type":"stream","text":["The system cannot find the file specified.\n"]}],"source":["%pip install 'keras<3.0.0' mediapipe-model-maker"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T04:22:00.874102Z","iopub.status.busy":"2024-04-30T04:22:00.873754Z","iopub.status.idle":"2024-04-30T04:22:10.689661Z","shell.execute_reply":"2024-04-30T04:22:10.688906Z","shell.execute_reply.started":"2024-04-30T04:22:00.874051Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\Yashk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torchvision.utils as vutils\n","import torchvision.transforms as transforms\n","import os\n","import sys\n","input_dir = r'D:\\clg files\\RecSys\\py_files'\n","sys.path.append(input_dir)\n","\n","from data_util import AttDesDataset\n","from utils import weights_init\n","\n","# from dcgan_model import Generator, Discriminator\n","import time\n","import imageio\n","import matplotlib.pyplot as plt\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T04:22:10.691416Z","iopub.status.busy":"2024-04-30T04:22:10.690874Z","iopub.status.idle":"2024-04-30T04:22:10.743811Z","shell.execute_reply":"2024-04-30T04:22:10.742709Z","shell.execute_reply.started":"2024-04-30T04:22:10.691388Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using Device cpu\n"]}],"source":["# Setting device to cuda\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(\"Using Device\", device)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T04:22:10.746034Z","iopub.status.busy":"2024-04-30T04:22:10.745559Z","iopub.status.idle":"2024-04-30T04:22:10.762543Z","shell.execute_reply":"2024-04-30T04:22:10.761792Z","shell.execute_reply.started":"2024-04-30T04:22:10.745986Z"},"trusted":true},"outputs":[],"source":["# directory to store output images\n","output_save_path = './generated_images/'\n","os.makedirs(output_save_path, exist_ok=True)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T04:22:10.764090Z","iopub.status.busy":"2024-04-30T04:22:10.763759Z","iopub.status.idle":"2024-04-30T04:22:10.770347Z","shell.execute_reply":"2024-04-30T04:22:10.769432Z","shell.execute_reply.started":"2024-04-30T04:22:10.764063Z"},"trusted":true},"outputs":[],"source":["# directory to store trained models\n","model_save_path = './saved_models/'\n","os.makedirs(model_save_path, exist_ok=True)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["from torch.utils.data import Dataset"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["from torch.nn.utils.rnn import pad_sequence\n","\n","class CustomDataset(Dataset):\n","    def __init__(self, data):\n","        self.images = data['images']\n","        self.captions = data['captions']    \n","        self.wrong_images = data['wrong_images']\n","\n","    def __getitem__(self, index):\n","        image = self.images[index]\n","        caption = self.captions[index]\n","        caption_tensor = torch.tensor(caption)\n","        wrong_images = self.wrong_images[index] \n","        return image, caption_tensor, wrong_images\n","\n","    def __len__(self):\n","        return len(self.images)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["def collate_fn(batch):\n","    images = [item[0] for item in batch]\n","    captions = [item[1] for item in batch]\n","\n","    # Stack images\n","    images = torch.stack(images, 0)\n","\n","    # Truncate or pad captions\n","    max_length = 128  # whatever fixed length\n","    captions = [cap[:max_length] for cap in captions]  # Truncate\n","    embedding_size = 768 \n","    captions = [torch.cat([cap, torch.zeros(max_length - cap.size(0), embedding_size)]) for cap in captions]\n","\n","    # Stack captions\n","    captions = torch.stack(captions, 0)\n","\n","    return images, captions"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["D:\\clg files\\RecSys\\data_files\\wrong_images\n"]}],"source":["from PIL import Image\n","\n","directory = r'D:\\clg files\\RecSys\\data_files\\wrong_images'\n","normalize = transforms.Normalize(mean=[0, 0, 0], std=[1, 1, 1])\n","transform = transforms.Compose([\n","                                            transforms.Resize((448,448)),\n","                                            transforms.RandomHorizontalFlip(),\n","                                            transforms.ToTensor(),\n","                                            normalize,\n","                                        ])\n","print(directory)\n","wrong_imgs = []\n","for filename in os.listdir(directory):\n","        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n","                full_path = os.path.join(directory, filename)\n","                img = Image.open(full_path)\n","                img = transform(img)\n","                wrong_imgs.append(img)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["488\n"]}],"source":["print(len(wrong_imgs))"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([3, 448, 448])"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["wrong_imgs[0].shape"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T04:22:10.775506Z","iopub.status.busy":"2024-04-30T04:22:10.775153Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["440\n","No of batches in train loader:  110\n","No of batches in validation loader:  22\n"]}],"source":["from torch.utils.data import DataLoader, SubsetRandomSampler\n","import numpy as np\n","\n","data_root = r'data_files\\smoll.csv'\n","split_root = ''\n","dataset_name = 'Furniture'\n","normalize = transforms.Normalize(mean=[0, 0, 0], std=[1, 1, 1])\n","batch_size = 4 #128\n","dataset = AttDesDataset(data_root, dataset_name, transform=transforms.Compose([\n","                                            transforms.Resize((448,448)),\n","                                            transforms.RandomHorizontalFlip(),\n","                                            transforms.ToTensor(),\n","                                            normalize,\n","                                        ]))\n","\n","temp_dataset = {'images': dataset.images, 'captions': dataset.descriptions, 'wrong_images': wrong_imgs}\n","custom_dataset = CustomDataset(temp_dataset)\n","print(len(custom_dataset))\n","# print(len(custom_dataset['captions']))\n","num_samples = len(custom_dataset)\n","indices = list(range(num_samples))\n","split = int(np.floor(0.8 * num_samples))  # 80-20 train-validation split\n","\n","# Randomly shuffle the indices\n","np.random.shuffle(indices)\n","\n","# Split the indices into train and validation sets\n","train_indices, val_indices = indices[:split], indices[split:]\n","\n","# Define samplers for train and validation sets\n","train_sampler = SubsetRandomSampler(train_indices)\n","val_sampler = SubsetRandomSampler(val_indices)\n","\n","# Define DataLoader for train and validation sets using samplers\n","train_loader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n","val_loader = DataLoader(custom_dataset, batch_size=batch_size, sampler=val_sampler, num_workers=0)\n","\n","# Check the number of batches in train and validation loaders\n","print(\"No of batches in train loader: \", len(train_loader))\n","print(\"No of batches in validation loader: \", len(val_loader))\n","\n"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["488\n"]}],"source":["print(len(wrong_imgs))"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"text/plain":["440"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["len(dataset.images)"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"data":{"text/plain":["440"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["len(dataset.descriptions)"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([3, 448, 448])"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["dataset.images[0].shape"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"data":{"text/plain":["100"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["len(dataset.descriptions[0])"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[ 0.0325,  0.0310, -0.2749,  ...,  0.1786, -0.0305, -0.0821],\n","        [ 0.3724,  1.5462,  0.4730,  ..., -1.5460, -1.3778, -0.4613],\n","        [ 0.9117, -0.3610,  0.1527,  ..., -0.8065,  1.4866,  0.3510],\n","        ...,\n","        [ 0.2698,  0.7917, -0.7626,  ...,  0.6083, -0.4508, -0.0309],\n","        [-0.5603,  0.8609, -1.1632,  ...,  1.6056, -0.3746, -0.0662],\n","        [-0.4179,  0.0190, -0.6336,  ...,  1.9185, -1.0130,  0.8874]])\n"]}],"source":["print(dataset.descriptions[0])"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["100\n"]}],"source":["print(len(temp_dataset['captions'][0]))"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"data":{"text/plain":["100"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["len(custom_dataset[1][1])"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["# for i, data in enumerate(train_loader):\n","#     print(type(data))  # This will print the type of the data element\n","#     if isinstance(data, list) or isinstance(data, tuple):\n","#         print(len(data))  \n","#     elif isinstance(data, dict):\n","#         print(data.keys())\n","#     if i >= 0: \n","#         break"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0\n","<class 'list'>\n","torch.Size([4, 3, 448, 448])\n","torch.Size([4, 100, 768])\n"]}],"source":["prev1 = 448\n","\n","for batch_idx,batch in enumerate(train_loader):\n","    print(batch_idx)\n","    print(type(batch))\n","    print(batch[0].shape)\n","    print(batch[1].shape)\n","    if batch_idx == 0:\n","        break   \n","    # if(batch[0].shape[2] != prev1 and batch[3].shape[0] != prev1):\n","    #     print(\"Batch size mismatch\")\n","    #     print(f'batch_idx: {batch_idx}, batch[0].shape: {batch[0].shape}, batch[1].shape: {batch[1].shape}')\n","    #     break"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["100\n"]}],"source":["print(len(custom_dataset.captions[0]))"]},{"cell_type":"code","execution_count":26,"metadata":{"trusted":true},"outputs":[],"source":["# setting up parameters\n","noise_dim = 100\n","embed_dim = 768\n","embed_out_dim = 100\n","batch_size = 4 #128\n","real_label = 1\n","fake_label = 0\n","learning_rate = 0.0002\n","l1_coef = 50\n","l2_coef = 100\n","\n","num_epochs = 10\n","log_interval = 18 #43"]},{"cell_type":"code","execution_count":27,"metadata":{"trusted":true},"outputs":[],"source":["# loss functions\n","criterion = nn.BCELoss()\n","l2_loss = nn.MSELoss()\n","l1_loss = nn.L1Loss()"]},{"cell_type":"code","execution_count":28,"metadata":{"trusted":true},"outputs":[],"source":["# lists to store losses\n","D_losses = []\n","G_losses = []"]},{"cell_type":"code","execution_count":29,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["Generator(\n","  (text_embedding): Sequential(\n","    (0): Linear(in_features=768, out_features=100, bias=True)\n","    (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n","  )\n","  (model): Sequential(\n","    (0): ConvTranspose2d(10100, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU(inplace=True)\n","    (3): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (5): ReLU(inplace=True)\n","    (6): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (8): ReLU(inplace=True)\n","    (9): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (10): Tanh()\n","  )\n",")"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["# initializing generator\n","from dcgan1 import Generator, Discriminator\n","\n","generator = Generator(channels=3, embed_dim=embed_dim, noise_dim=noise_dim, embed_out_dim=embed_out_dim).to(device)\n","generator.apply(weights_init)"]},{"cell_type":"code","execution_count":30,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["Discriminator(\n","  (model): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n","  )\n","  (text_embedding): Embedding(\n","    (text_embedding): Sequential(\n","      (0): Linear(in_features=768, out_features=100, bias=True)\n","      (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): LeakyReLU(negative_slope=0.2, inplace=True)\n","    )\n","  )\n","  (output): Sequential(\n","    (0): Conv2d(1024, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n","    (1): Sigmoid()\n","  )\n",")"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["# initializing discriminator\n","discriminator = Discriminator(channels=3, embed_dim=embed_dim, embed_out_dim=embed_out_dim).to(device)\n","discriminator.apply(weights_init)"]},{"cell_type":"code","execution_count":31,"metadata":{"trusted":true},"outputs":[],"source":["# setting up Adam optimizer for Generator and Discriminator\n","optimizer_G = torch.optim.Adam(generator.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n","optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=learning_rate, betas=(0.5, 0.999))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":32,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 1\n","epoch: 1, batch: 1/110\n","epoch: 1, batch: 2/110\n","epoch: 1, batch: 3/110\n","epoch: 1, batch: 4/110\n","epoch: 1, batch: 5/110\n","epoch: 1, batch: 6/110\n","epoch: 1, batch: 7/110\n","epoch: 1, batch: 8/110\n","epoch: 1, batch: 9/110\n","epoch: 1, batch: 10/110\n","epoch: 1, batch: 11/110\n","epoch: 1, batch: 12/110\n","epoch: 1, batch: 13/110\n","epoch: 1, batch: 14/110\n","epoch: 1, batch: 15/110\n","epoch: 1, batch: 16/110\n","epoch: 1, batch: 17/110\n","epoch: 1, batch: 18/110\n","Epoch 1 [18/110] loss_D: 1.6885 loss_G: 49.5569 time: 84.18\n","epoch: 1, batch: 19/110\n","epoch: 1, batch: 20/110\n","epoch: 1, batch: 21/110\n","epoch: 1, batch: 22/110\n","epoch: 1, batch: 23/110\n","epoch: 1, batch: 24/110\n","epoch: 1, batch: 25/110\n","epoch: 1, batch: 26/110\n","epoch: 1, batch: 27/110\n","epoch: 1, batch: 28/110\n","epoch: 1, batch: 29/110\n","epoch: 1, batch: 30/110\n","epoch: 1, batch: 31/110\n","epoch: 1, batch: 32/110\n","epoch: 1, batch: 33/110\n","epoch: 1, batch: 34/110\n","epoch: 1, batch: 35/110\n"]}],"source":["# training loop\n","import torch.nn.functional as F\n","import PIL\n","# iterating over number of epochs\n","from datetime import date\n","from datetime import timedelta\n","start_time = time.time()\n","for epoch in range(num_epochs):\n","    \n","    batch_time = time.time()\n","    print('Epoch: {}'.format(epoch+1))\n","    #iterating over each batch\n","    for batch_idx,batch in enumerate(train_loader):   \n","\n","        print(f'epoch: {epoch+1}, batch: {batch_idx+1}/{len(train_loader)}')\n","        # print(batch_idx)\n","        \n","        # reading the data into variables and moving them to device\n","        images = batch[0].to(device)\n","        # print('hi after images')\n","        wrong_images = batch[2].to(device)\n","        # print('hi after wrong images')\n","        embeddings = batch[1].to(device).float()\n","\n","        batch_size = images.size(0)\n","        # print(f'embeddings size: {embeddings.size()}')\n","        # ================================================================== #\n","        #                      Train the discriminator                       #\n","        # ================================================================== #\n","        # print('training the descriminator')\n","        # Clear gradients for the discriminator\n","        optimizer_D.zero_grad()\n","        \n","        # Generate random noise\n","        noise_dim = 100  \n","        batch_size = images.size(0)\n","        noise = torch.randn(embeddings.size(0), noise_dim).to(device)\n","        # noise = noise.view(embeddings.size(0), noise_dim, 1, 1)\n","   \n","        # Generate fake image batch with the generator\n","        # print(f'noise size: {noise.size()}')\n","        # print(f'embeddings size: {embeddings.size()}')\n","        \n","        fake_images = generator(noise, embeddings)\n","        # print('hi after fake images')\n","        # if batch_idx == 0:\n","            # print(f'noise size: {noise.size()}')\n","            # print(f'embeddings size: {embeddings.size()}')\n","            # print(f'fake_images size: {fake_images.size()}')\n","        fake_images = F.interpolate(fake_images, size=(448, 448), mode='bilinear', align_corners=False)\n","        # print('fake images after interpolation: ', fake_images.size())\n","        # Forward pass real batch and calculate loss\n","\n","        real_out, real_act = discriminator(images, embeddings)\n","        d_loss_real = criterion(real_out, torch.full_like(real_out, real_label, device=device))\n","        \n","        # Forward pass wrong batch and calculate loss\n","        wrong_out, wrong_act = discriminator(wrong_images, embeddings)\n","        d_loss_wrong = criterion(wrong_out, torch.full_like(wrong_out, fake_label, device=device))\n","        \n","        # Forward pass fake batch and calculate loss\n","        fake_out, fake_act = discriminator(fake_images.detach(), embeddings)\n","        d_loss_fake = criterion(fake_out, torch.full_like(fake_out, fake_label, device=device))\n","        \n","        # Compute total discriminator loss\n","        d_loss = d_loss_real + d_loss_wrong + d_loss_fake\n","        \n","        # Backpropagate the gradients\n","        d_loss.backward()\n","        \n","        # Update the discriminator\n","        optimizer_D.step()\n","        \n","        # ================================================================== #\n","        #                        Train the generator                         #\n","        # ================================================================== #\n","        # print('Training the generator')\n","        # Clear gradients for the generator\n","        optimizer_G.zero_grad()\n","       \n","        # Generate new fake images using Generator\n","        fake_images = generator(noise, embeddings)\n","        # print(f'fake_images size: {fake_images.size()}')\n","        fake_images = F.interpolate(fake_images, size=(448, 448), mode='bilinear', align_corners=False)\n","        # print('fake images after interpolation: ', fake_images.size())\n","        # Get discriminator output for the new fake images\n","        out_fake, act_fake = discriminator(fake_images, embeddings)\n","        # Get discriminator output for the real images\n","        out_real, act_real = discriminator(images, embeddings)\n","        \n","        # Calculate losses\n","        g_bce = criterion(out_fake, torch.full_like(out_fake, real_label, device=device)) \n","        g_l1 = l1_coef * l1_loss(fake_images, images)\n","        g_l2 = l2_coef * l2_loss(torch.mean(act_fake, 0), torch.mean(act_real, 0).detach())\n","        \n","        # Compute total generator loss\n","        g_loss = g_bce + g_l1 + g_l2\n","        \n","        # Backpropagate the gradients\n","        g_loss.backward()\n","        \n","        # Update the generator\n","        optimizer_G.step()\n","        \n","        # adding loss to the list\n","        D_losses.append(d_loss.item())\n","        G_losses.append(g_loss.item())\n","        \n","        # progress based on log_interval\n","        if (batch_idx+1) % log_interval == 0 and batch_idx > 0:\n","            print('Epoch {} [{}/{}] loss_D: {:.4f} loss_G: {:.4f} time: {:.2f}'.format(\n","                          epoch+1, batch_idx+1, len(train_loader),\n","                          d_loss.mean().item(),\n","                          g_loss.mean().item(),\n","                          time.time() - batch_time))\n","        \n","        # storing generator output after every 10 epochs\n","        if batch_idx == len(train_loader)-1 and ((epoch+1)%10==0 or epoch==0):\n","            viz_sample = torch.cat((images[:32], fake_images[:32]), 0)\n","            current_time = datetime.now().strftime(\"%H_%M_%S\")\n","            vutils.save_image(viz_sample,\n","            os.path.join(output_save_path, 'output_{}_epoch_{}_time_{}.png'.format(date.today(),epoch+1, current_time)),\n","                            nrow=8,normalize=True)\n","            # viz_sample.show()\n","\n","\n","total_time = time.time() - start_time\n","formatted_time = str(timedelta(seconds=total_time))\n","\n","print('Total train time: {}'.format(formatted_time))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# saving the trained models\n","torch.save(generator.state_dict(), os.path.join(model_save_path, 'generator_{}.pth'.format(date.today(),epoch+1)))\n","torch.save(discriminator.state_dict(), os.path.join(model_save_path,'discriminator_{}.pth'.format(date.today(),epoch+1)))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# generator loss plot\n","plt.figure(figsize=(10,5))\n","plt.title(\"Generator Loss During Training\")\n","plt.plot(G_losses)\n","plt.xlabel(\"iterations\")\n","plt.ylabel(\"Loss\")\n","plt.show()\n","\n","plt.savefig(os.path.join(output_save_path, 'output_generatorLoss_{}.png'.format(date.today())))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# discriminator loss plot\n","plt.figure(figsize=(10,5))\n","plt.title(\"Discriminator Loss During Training\")\n","plt.plot(D_losses)\n","plt.xlabel(\"iterations\")\n","plt.ylabel(\"Loss\")\n","plt.show()\n","\n","plt.savefig(os.path.join(output_save_path, 'output_discriminatorLoss_{}.png'.format(date.today())))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torchvision.utils as vutils\n","import os\n","from datetime import date\n","\n","# Assuming output_save_path and epoch are defined\n","output_save_path = './'\n","epoch = 1\n","\n","vutils.save_image(viz_sample, os.path.join(output_save_path, 'output_{}_epoch_{}.png'.format(date.today(), epoch+1)), normalize=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["viz_sample = torch.cat((images[:32], fake_images[:32]), 0)\n","vutils.save_image(viz_sample,\n","                  os.path.join(output_save_path, 'output_{}_epoch_{}.png'.format(date.today(),epoch+1)),\n","                              nrow=8,normalize=True)\n","# viz_sample.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from PIL import Image\n","import torchvision.transforms as transforms\n","\n","# Assuming viz_sample is your tensor\n","# First, detach and move the tensor to cpu\n","viz_sample = viz_sample.detach().cpu()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Convert the tensor to a PIL Image\n","first_image = viz_sample[0]\n","viz_sample_pil = transforms.ToPILImage()(first_image)\n","\n","# Display the image\n","viz_sample_pil.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Convert the tensor to a PIL Image\n","first_image = viz_sample[1]\n","viz_sample_pil = transforms.ToPILImage()(first_image)\n","\n","# Display the image\n","viz_sample_pil.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Convert the tensor to a PIL Image\n","first_image = viz_sample[2]\n","viz_sample_pil = transforms.ToPILImage()(first_image)\n","\n","# Display the image\n","viz_sample_pil.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Convert the tensor to a PIL Image\n","first_image = viz_sample[3]\n","viz_sample_pil = transforms.ToPILImage()(first_image)\n","\n","# Display the image\n","viz_sample_pil.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from pytorch_pretrained_bert.tokenization import BertTokenizer\n","from pytorch_pretrained_bert.modeling import BertModel\n","\n","def get_embeddings(text):\n","    tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n","    bert_model = BertModel.from_pretrained('bert-base-chinese')\n","    des_len = 100\n","    tokens = [\"[CLS]\"]\n","    token_obj = tokenizer.tokenize(text)\n","    tokens.extend(token_obj)\n","    tokens.append(\"[SEP]\")\n","    tokens = tokenizer.convert_tokens_to_ids(tokens)\n","\n","        # Pad or truncate to the desired length\n","    if len(tokens) < des_len:\n","        tokens += [0] * (des_len - len(tokens))  # Pad with zeros\n","    else:\n","        tokens = tokens[:des_len]  # Truncate to the maximum length\n","    \n","\n","    input_ids = tokens\n","    input_ids = torch.tensor(input_ids).unsqueeze(0)\n","\n","     # Get BERT outputs\n","    with torch.no_grad():\n","                \n","        input_ids = torch.tensor(input_ids)\n","        # print(type(input_ids))\n","        outputs = bert_model(input_ids)\n","            \n","    output_tensor = outputs[0]\n","    # Get the vector of the [CLS] token (first token)\n","    cls_vector = output_tensor[0][0][:]\n","    return cls_vector\n"," "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","def generate_image(caption):\n","    # Convert caption to embedding (if needed) and generate noise vector\n","    # You need to define how to convert the caption to an embedding\n","    # For example, you can use a pre-trained language model like BERT to get embeddings\n","    \n","    # Generate fake image using Generator\n","    with torch.no_grad():\n","        # Assuming you have a function to convert caption to embeddings\n","        embeddings = get_embeddings(caption)\n","        print(f'embeddings size: {embeddings.size()}')\n","        noise = torch.randn(batch_size, embeddings.size(0))\n","        print(f'noise size: {noise.size()}')\n","        embeddings = embeddings.unsqueeze(0)\n","        embeddings = embeddings.repeat(batch_size, 1, 1)\n","        fake_image = generator(noise, embeddings)\n","        fake_image = F.interpolate(fake_image, size=(448, 448), mode='bilinear', align_corners=False)\n","    \n","    return fake_image"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from translate import Translator\n","\n","def translate_to_chinese(text):\n","    translator = Translator(to_lang=\"zh\")\n","    translation = translator.translate(text)\n","    return translation\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["caption = \"A black room with no furniture\"\n","chinese_caption = translate_to_chinese(caption)\n","print(f'chinese_caption: {chinese_caption}')\n","generated_image = generate_image(chinese_caption)   \n","transforms.ToPILImage()(generated_image[1]).show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4891975,"sourceId":8245670,"sourceType":"datasetVersion"},{"datasetId":4892017,"sourceId":8245726,"sourceType":"datasetVersion"},{"datasetId":4908067,"sourceId":8267485,"sourceType":"datasetVersion"},{"datasetId":4908285,"sourceId":8267780,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":4}
